{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads wavefunctions and potential from their respective files\n",
    "def load_dataset():\n",
    "    with open('ian_potentials.txt', 'r') as f:\n",
    "        potential = np.loadtxt(f, delimiter = ', ')\n",
    "        #print(np.shape(potential.T))\n",
    "\n",
    "    with open('ian_wavefunctions.txt', 'r') as f:\n",
    "        wavefunction = np.loadtxt(f, delimiter = ', ')\n",
    "        #print(np.shape(wavefunction.T))\n",
    "    \n",
    "    return potential.T, wavefunction.T \n",
    "        \n",
    "#X: potentials. X because it will the input \n",
    "#Y: wavefunctions. Y because it will be our labels\n",
    "\n",
    "X, Y = load_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets start with initialization. \n",
    "def initialize_parameters(n_x,n_y): # this is where we decide the NN architecture. Let's take n_x as an argument, since our first layer weights need to deal with that. Same idea with n_y\n",
    "   \n",
    "    parameters = {'W1':None,\n",
    "                  'b1':None,\n",
    "                  'W2':None,\n",
    "                  'b2':None,\n",
    "                  'W3':None,\n",
    "                  'b3':None\n",
    "                 }\n",
    "    \n",
    "    # randomly initializing parameters to be in Gaussian distribution, with mean 0 and variance 1\n",
    "    parameters['W1'] = np.random.randn(5,n_x)*np.sqrt(2/(n_x)) # actually initializing w He initialization\n",
    "    parameters['b1'] = np.random.randn(5,1)*np.sqrt(2/(n_x))\n",
    "    parameters['W2'] = np.random.randn(6,5)*np.sqrt(2/(5))\n",
    "    parameters['b2'] = np.random.randn(6,1)*np.sqrt(2/(5))\n",
    "    parameters['W3'] = np.random.randn(n_y,6)*np.sqrt(2/(6))\n",
    "    parameters['b3'] = np.random.randn(n_y,1)*np.sqrt(2/(6))\n",
    "    return parameters\n",
    "\n",
    "def initialize_gradients(parameters):\n",
    "    gradients={\n",
    "        'dW1':None,\n",
    "        'db1':None,\n",
    "        'dW2':None,\n",
    "        'db2':None,\n",
    "        'dW3':None,\n",
    "        'db3':None\n",
    "    }\n",
    "    gradients['dW1'] = np.zeros(np.shape(parameters['W1']))\n",
    "    gradients['db1'] = np.zeros(np.shape(parameters['b1']))\n",
    "    gradients['dW2'] = np.zeros(np.shape(parameters['W2']))\n",
    "    gradients['db2'] = np.zeros(np.shape(parameters['b2']))\n",
    "    gradients['dW3'] = np.zeros(np.shape(parameters['W3']))\n",
    "    gradients['db3'] = np.zeros(np.shape(parameters['b3']))\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax allows us to denote probabilities because the total sum at the end will all add up to one\n",
    "def softmax(ZL): # inputs the vector Z of the final layer\n",
    "    denom = np.sum((np.exp(ZL-np.max(ZL))),axis=0) # subtract max(x) to shrink values & avoid exploding gradient\n",
    "    return np.divide((np.exp(ZL-np.max(ZL))),denom)\n",
    "def sigmoid(ZL): # NOTE: decided to go w relu on all just bc it's easier to do derivatives\n",
    "    return 1/(1+np.exp(-ZL))\n",
    "#We will probably just use relu \n",
    "def relu(ZL):\n",
    "    return ZL.clip(min=0) # clips all negative elements of ZL, setting them to 0\n",
    "def relu_back(AL): # computes derivative of relu fn, given the layer's activation\n",
    "    mask = np.zeros(np.shape(AL))\n",
    "    mask[AL>0]=1 # NOTE: technically, should be 1 or 0 based on ZL, not AL. But since AL = 0 iff ZL <= 0, this still works as shorthand\n",
    "    mask[AL<=0]=0\n",
    "#     print('AL is: ' +str(AL))\n",
    "#     print('mask is: '+str(mask))\n",
    "    return mask # return original value of AL if entry was nonnegative; 0 if negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X,Y,parameters,lambd=0): # given a set of parameters and our data, let's pass it forward and see what our guesses are.\n",
    "    m = np.shape(X)[1]\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    #Z1 would be the activations of the entire layer. 'Vectorizing' saves alot of run time. \n",
    "    Z1 = np.dot(W1,X)+b1\n",
    "    A1 = relu(Z1)\n",
    "    \n",
    "    Z2 = np.dot(W2,A1)+b2\n",
    "    A2 = relu(Z2)\n",
    "\n",
    "    Z3 = np.dot(W3,A2)+b3\n",
    "    A3 = softmax(Z3)\n",
    "\n",
    "    assert np.shape(A3) == np.shape(Y)\n",
    "\n",
    "    cost = 1/m*(np.sum(np.multiply(Y,-np.log(A3))) + 0.5*lambd*(np.sum(W3**2)+np.sum(W2**2)+np.sum(W1**2)))# let's do the cross entropy loss\n",
    "\n",
    "    cache = (Z1,Z2,Z3,X,A1,A2,lambd) # saving these babies for later\n",
    "    \n",
    "    return A3,cache,cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(A3,Y,cache, parameters,gradients,debug=False):    # Note: A3, Y, and cache can be of however many examples we happen to want to input at once\n",
    "    (Z1,Z2,Z3,X,A1,A2,lambd) = cache\n",
    "    m = np.shape(X)[1]\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    #We dont really care about dC/dz3 but this first derivative makes dC/dw3 much easier. dC/dz is given by the link. \n",
    "    dZ3 = A3-Y # someone calculated this here: https://deepnotes.io/softmax-crossentropy\n",
    "    dW3 = 1/m*(np.dot(dZ3,A2.T) + lambd*np.sum(abs(W3)))\n",
    "    db3 = 1/m*np.sum(dZ3,axis=1,keepdims=True) # so we don't get any rank 1 arrays\n",
    "\n",
    "    if debug:\n",
    "        print('Layer 3 Backprop:')\n",
    "#         print('dA3 = '+str(np.average(dA3)))\n",
    "        print('dZ3 = '+str(np.average(dZ3)))\n",
    "        print('dW3 = '+str(np.average(dW3)))\n",
    "        print('db3 = '+str(np.average(db3)))\n",
    "        print('~'*30)\n",
    "    \n",
    "    #relu_back is the derivative of relu() \n",
    "    #It is easier to plug in dz/dw rather than dA/dw\n",
    "    dA2 = np.dot(W3.T,dZ3) # basically running fwd prop but in reverse! using W3.T to get from dZ3 to A2, instead of the other way around! :D\n",
    "    dZ2 = np.multiply(dA2,relu_back(A2)) # derivative of the relu fn\n",
    "    dW2 = 1/m*(np.dot(dZ2,A1.T) + lambd*np.sum(abs(W2)))# include regulaization term\n",
    "    db2 = 1/m*np.sum(dZ2,axis=1,keepdims=True)\n",
    "    \n",
    "    if debug:\n",
    "        print('Layer 2 Backprop:')\n",
    "#         print('dA2 = '+str(np.average(dA2)))\n",
    "        print('dZ2 = '+str(np.average(dZ2)))\n",
    "        print('dW2 = '+str(np.average(dW2)))\n",
    "        print('db2 = '+str(np.average(db2)))\n",
    "        print('~'*30)\n",
    "    #We can basically copy the code for dA2\n",
    "    dA1 = np.dot(W2.T,dZ2)\n",
    "    dZ1 = np.multiply(dA1,relu_back(A1))\n",
    "    dW1 = 1/m*(np.dot(dZ1,X.T) + lambd*np.sum(abs(W1)))\n",
    "    db1 = 1/m*np.sum(dZ1,axis=1,keepdims=True)\n",
    "    \n",
    "    if debug:\n",
    "        print('Layer 1 Backprop:')\n",
    "#         print('dA1 = '+str(np.average(dA1)))\n",
    "        print('dZ1 = '+str(np.average(dZ1)))\n",
    "        print('dW1 = '+str(np.average(dW1)))\n",
    "        print('db1 = '+str(np.average(db1)))\n",
    "        print('~'*30)\n",
    "    \n",
    "    assert (np.shape(W1)==np.shape(dW1))\n",
    "    assert (np.shape(b1)==np.shape(db1))\n",
    "    assert (np.shape(W2)==np.shape(dW2))\n",
    "    assert (np.shape(b2)==np.shape(db2))\n",
    "    assert (np.shape(W3)==np.shape(dW3))\n",
    "    assert (np.shape(b3)==np.shape(db3))\n",
    "    \n",
    "    \n",
    "    gradients['dW1'] = dW1\n",
    "    gradients['db1'] = db1\n",
    "    gradients['dW2'] = dW2\n",
    "    gradients['db2'] = db2\n",
    "    gradients['dW3'] = dW3\n",
    "    gradients['db3'] = db3\n",
    "    \n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters,gradients,learning_rate=0.005,clip=False):\n",
    "    if clip:\n",
    "        parameters['W1'] = parameters['W1'] - learning_rate*np.clip(gradients['dW1'],-5,5)\n",
    "        parameters['b1'] = parameters['b1'] - learning_rate*np.clip(gradients['db1'],-5,5)\n",
    "        parameters['W2'] = parameters['W2'] - learning_rate*np.clip(gradients['dW2'],-5,5)\n",
    "        parameters['b2'] = parameters['b2'] - learning_rate*np.clip(gradients['db2'],-5,5)\n",
    "        parameters['W3'] = parameters['W3'] - learning_rate*np.clip(gradients['dW3'],-5,5)\n",
    "        parameters['b3'] = parameters['b3'] - learning_rate*np.clip(gradients['db3'],-5,5)  \n",
    "    else:\n",
    "        parameters['W1'] = parameters['W1'] - learning_rate*gradients['dW1']\n",
    "        parameters['b1'] = parameters['b1'] - learning_rate*gradients['db1']\n",
    "        parameters['W2'] = parameters['W2'] - learning_rate*gradients['dW2']\n",
    "        parameters['b2'] = parameters['b2'] - learning_rate*gradients['db2']\n",
    "        parameters['W3'] = parameters['W3'] - learning_rate*gradients['dW3']\n",
    "        parameters['b3'] = parameters['b3'] - learning_rate*gradients['db3']\n",
    "    return parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
